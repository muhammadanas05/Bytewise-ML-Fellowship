{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Car Reviews with LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Car-ing is sharing**, an auto dealership company for car sales and rental, is taking their services to the next level thanks to **Large Language Models (LLMs)**.\n",
    "\n",
    "As their newly recruited AI and NLP developer, you've been asked to prototype a chatbot app with multiple functionalities that not only assist customers but also provide support to human agents in the company.\n",
    "\n",
    "The solution should receive textual prompts and use a variety of pre-trained Hugging Face LLMs to respond to a series of tasks, e.g. classifying the sentiment in a carâ€™s text review, answering a customer question, summarizing or translating text, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('car_reviews.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am very satisfied with my 2014 Nissan NV SL....</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The car is fine. It's a bit loud and not very ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My first foreign car. Love it, I would buy ano...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've come across numerous reviews praising the...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been dreaming of owning an SUV for quite ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review     Class\n",
       "0  I am very satisfied with my 2014 Nissan NV SL....  POSITIVE\n",
       "1  The car is fine. It's a bit loud and not very ...  NEGATIVE\n",
       "2  My first foreign car. Love it, I would buy ano...  POSITIVE\n",
       "3  I've come across numerous reviews praising the...  NEGATIVE\n",
       "4  I've been dreaming of owning an SUV for quite ...  POSITIVE"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from transformers import pipeline\n",
    "\n",
    "# Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Initialize sentiment analysis pipeline with a specific model and revision\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", revision=\"af0f99b\")\n",
    "\n",
    "# Perform sentiment analysis on the reviews\n",
    "results = classifier(df['Review'].tolist())\n",
    "\n",
    "# Extract the labels from results and store them in predicted_labels\n",
    "predicted_labels = [result['label'] for result in results]\n",
    "\n",
    "# Map the labels to binary integer labels: POSITIVE -> 1, NEGATIVE -> 0\n",
    "predictions = [1 if label == \"POSITIVE\" else 0 for label in predicted_labels]\n",
    "\n",
    "# Display predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8571428571428571)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load accuracy and F1 score metrics    \n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# True labels from the dataset\n",
    "true_labels = [1 if label == \"POSITIVE\" else 0 for label in df['Class']]\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_result_dict = accuracy.compute(references=true_labels, predictions=predictions)\n",
    "accuracy_result = accuracy_result_dict['accuracy']\n",
    "f1_result_dict = f1.compute(references=true_labels, predictions=predictions)\n",
    "f1_result = f1_result_dict['f1']\n",
    "\n",
    "# Display accuracy and F1 score\n",
    "accuracy_result, f1_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation and BLEU Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model translation:\n",
      "Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta camioneta para mis entregas de negocios y uso personal.\n",
      "Spanish translation references:\n",
      "['Estoy muy satisfecho con mi Nissan NV SL 2014. Utilizo esta camioneta para mis entregas comerciales y uso personal.', 'Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta furgoneta para mis entregas comerciales y uso personal.']\n",
      "BLEU score: 0.7794483794144497\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "# Load the translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "# Extract the first two sentences from the first review\n",
    "first_review = df['Review'][0].split('.')[:2]\n",
    "first_review_text = '. '.join(first_review) + '.'\n",
    "\n",
    "# Increase max_length to handle longer input texts\n",
    "translated_review = translator(first_review_text, max_length=400)[0]['translation_text']\n",
    "print(f\"Model translation:\\n{translated_review}\")\n",
    "\n",
    "# Load reference translations from the file\n",
    "with open('reference_translations.txt', 'r') as f:\n",
    "    reference_translations = f.readlines()\n",
    "\n",
    "# Strip newline characters from reference translations\n",
    "references = [line.strip() for line in reference_translations]\n",
    "print(f\"Spanish translation references:\\n{references}\")\n",
    "\n",
    "# Load and calculate BLEU score metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(predictions=[translated_review], references=[references])\n",
    "print(f\"BLEU score: {bleu_score['bleu']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Question Answering with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ride quality, reliability\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "# Define the question and context (2nd review)\n",
    "question = \"What did he like about the brand?\"\n",
    "context = df['Review'][1]\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Get the answer from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Find the start and end token indices of the answer\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "# Get the tokens for the answer span\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "# Decode the answer tokens to a string\n",
    "answer = tokenizer.decode(answer_span, skip_special_tokens=True)\n",
    "\n",
    "# Display the answer\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text:\n",
      "the Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. I have hauled 12 bags of mulch in the back with the seats down and could have held more.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# Suppress the symlink warning\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Load the summarization pipeline with the new model\n",
    "model_name = \"cnicu/t5-small-booksum\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "# Get the last review (5th one) to summarize\n",
    "text_to_summarize = df['Review'].iloc[-1]\n",
    "\n",
    "# Summarize the text\n",
    "outputs = summarizer(text_to_summarize, max_length=53, min_length=50, do_sample=False)\n",
    "summarized_text = outputs[0]['summary_text']\n",
    "\n",
    "# Display the summarized text\n",
    "print(f\"Summarized text:\\n{summarized_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
