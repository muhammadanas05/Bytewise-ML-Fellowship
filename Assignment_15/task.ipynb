{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import Accuracy, Precision, Recall\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketDataset(Dataset):\n",
    "    def __init__(self, text_file, label_file, word_file):\n",
    "        with open(text_file, 'r') as f:\n",
    "            self.texts = json.load(f)\n",
    "        self.labels = np.load(label_file)\n",
    "        with open(word_file, 'r') as f:\n",
    "            self.word_to_idx = json.load(f)\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "        self.max_length = max(len(text) for text in self.texts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Convert words to indices\n",
    "        text_indices = [self.word_to_idx.get(word, 0) for word in text]\n",
    "        # Pad or truncate text\n",
    "        text_indices = text_indices[:self.max_length] + [0] * (self.max_length - len(text_indices))\n",
    "        return torch.tensor(text_indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TicketDataset('text.json', 'labels.npy', 'words.json')\n",
    "train_loader = DataLoader(train_dataset, batch_size=400, shuffle=True)\n",
    "\n",
    "test_dataset = TicketDataset('text.json', 'labels.npy', 'words.json')  \n",
    "test_loader = DataLoader(test_dataset, batch_size=400, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, target_size=5):\n",
    "        super(TicketClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(embed_dim, target_size)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        conved = F.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2) \n",
    "        return self.fc(conved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model, criterion, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = train_dataset.vocab_size\n",
    "target_size = len(np.unique(train_dataset.labels))  \n",
    "model = TicketClassifier(vocab_size=vocab_size, embed_dim=64, target_size=target_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = Accuracy(num_classes=target_size, task='multiclass')\n",
    "precision_metric = Precision(num_classes=target_size, average='none', task='multiclass')\n",
    "recall_metric = Recall(num_classes=target_size, average='none', task='multiclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed\n",
      "Epoch 2/3 completed\n",
      "Epoch 3/3 completed\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update metrics\n",
    "        accuracy_metric.update(predicted, labels)\n",
    "        precision_metric.update(predicted, labels)\n",
    "        recall_metric.update(predicted, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_metric.compute().item()\n",
    "precision = precision_metric.compute().tolist()\n",
    "recall = recall_metric.compute().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7070\n",
      "Precision (per class): [0.5272727012634277, 0.6125186085700989, 0.7132974863052368, 0.6861423254013062, 0.9469122290611267]\n",
      "Recall (per class): [0.4059999883174896, 0.41100001335144043, 0.9279999732971191, 0.9160000085830688, 0.8740000128746033]\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision (per class): {precision}')\n",
    "print(f'Recall (per class): {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'accuracy': accuracy, 'precision': precision, 'recall': recall}, 'metrics02.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
